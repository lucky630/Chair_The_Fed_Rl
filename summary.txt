## understanding doc for FIFA game.
flow- main-->load_model()-->FIFA(-->grab_screen,CNN)-->train

(ExperienceReplay),test.

--main--

load the pretrained model and weights first.
create the Fifa class obect
call train or test then plot the wins
--Fifa--

initialize the Cnn class for gettingthe state values.
get_reward() method to get game screen and reward screen
then use pytes to get score compare with previous score

then gave reward as 1-score increase,0-no score increase &

-1 game over. 3. is_over() check game is over or not using action=[0,1]. 4. observe() grab_screen then check for restart text if

present then start the game and get the new state. 5. act() choose one of the action then press the key

according to chosen action then wait for some time then

call get_reward(action) for reward,_is_over(action) for

is_over status,call observe() for next state.

--CNN--

get the saved frozen detection graph and image then try
to get the state which is 128 length array.

--train--

set the win_cnt,win_hist,epsilon(depend upon epoch
number)-used to select random action or not. 2. call the observe to get current screen. 3. loop through the game untill it will not game_over 4. take action randomly sometime or predict the action

based upon state. 5. do the action get the next state,reward and game_over

status, if reward is 1 increment win_count 6. save the experience in form of ([current

sate,action,reward,next state],game_over). 7. call get_batch() from Experience Replay that gave you

inputs and targets. 8. use that inputs and targets to do train_on_batch to get

loss.save model and return win_history.

--Exp Replay--

init() initialize max_memory,memory,discount
remember() will save states,game_over in the memory & if
memory size increase then delete the last one. 3. get_batch() create inputs(batch_size,env_dim-128) and

targets(inputs.shape[0],num_actions) 4. select the random experience from memory and see if game

over is for that or not. 5. from selected random experience get the state and put it

into inputs and predict target for that state. 6. predict the next state reward from that random

experience and get max of that to apply to the r+gamma*max

Q(s',a') formula for saving as target for the not game_over

condition. 7. return that inputs and targets.

--grabscreencoordinates--

get the part of the screen and then using pytes convert
to string and then show both image as well as result untill

press 'P'.

grabscreen,directkeys,getkeys are using win32 library to
grab and control window controls.

understand how DQN work deeply.
